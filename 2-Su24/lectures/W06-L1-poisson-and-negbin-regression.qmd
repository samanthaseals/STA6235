---
title: "Poisson and Negative Binomial Regressions"
subtitle: "STA6235: Modeling in Regression"
execute:
  echo: true
  warning: false
  message: false
  error: true
format: 
  revealjs:
    theme: uwf2
    embed-resources: true
    slide-number: false
    width: 1600
    height: 900
    df-print: paged
    html-math-method: katex
title-slide-attributes:
    data-background-image: /Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/title.png
    data-background-size: contain 
editor: source
pdf-separate-fragments: true
fig-align: center
---

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Suppose we are faced with *count* data.

    - This is discrete data, not continuous.

- Fortunately, the Poisson distribution is appropriate for count data. 

- The Poisson regression model is as follows:
  
$$\ln\left( y \right) = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k$$

- Note that this is similar to logistic regression in that we are using the natural log when modeling the outcome. 

## Today's Data {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Today we will be examining [US House election results](https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-11-07/readme.md) from 1976-2022, distributed by Tidy Tuesday.

::: {.panel-tabset}

## Data

```{r}
#| echo: false
library(tidyverse)
library(fastDummies)

house <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-11-07/house.csv') %>%
  filter(stage == "GEN" &
         party %in% c("REPUBLICAN", "DEMOCRAT") &
         state != "DISTRICT OF COLUMBIA") %>%
  mutate(candidatevotes = candidatevotes + 1,
         year10 = year/10,
         region = if_else(state %in% c("CONNECTICUT", "MAINE", "MASSACHUSETTS", "NEW HAMPSHIRE", "RHODE ISLAND", "VERMONT", "NEW JERSEY", "NEW YORK", "PENNSYLVANIA"), "Northeast",
                          if_else(state %in% c("ILLINOIS", "INDIANA", "MICHIGAN", "OHIO", "WISCONSIN", "IOWA", "KANSAS", "MINNESOTA", "MISSOURI", "NEBRASKA", "NORTH DAKOTA", "SOUTH DAKOTA"), "Midwest",
                                  if_else(state %in% c("DELAWARE", "FLORIDA", "GEORGIA", "MARYLAND", "NORTH CAROLINA", "SOUTH CAROLINA", "VIRGINIA","WEST VIRGINIA", "ALABAMA", "KENTUCKY", "MISSISSIPPI", "TENNESSEE", "ARKANSAS", "LOUISIANA", "OKLAHOMA", "TEXAS"), "South", "West")))) %>%
  dummy_cols(select_columns = c("region")) %>%
  filter(totalvotes < 1000000)

head(house)
```

## Variables of Interest

- **candidatevotes:** votes received by this candidate for this particular party

- **totalvotes:** total number of votes cast for this election

- **year:**	year in which election was held

- **region:** region that state belongs to, as defined by the US Census Bureau

    - I derived this; please examine the code to see how.

## Code

```{r}
#| eval: false
library(tidyverse)
library(fastDummies)

house <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-11-07/house.csv') %>%
  filter(stage == "GEN" &
         party %in% c("REPUBLICAN", "DEMOCRAT") &
         state != "DISTRICT OF COLUMBIA") %>%
  mutate(candidatevotes = candidatevotes + 1,
         year10 = year/10,
         region = if_else(state %in% c("CONNECTICUT", "MAINE", "MASSACHUSETTS", "NEW HAMPSHIRE", "RHODE ISLAND", "VERMONT", "NEW JERSEY", "NEW YORK", "PENNSYLVANIA"), "Northeast",
                          if_else(state %in% c("ILLINOIS", "INDIANA", "MICHIGAN", "OHIO", "WISCONSIN", "IOWA", "KANSAS", "MINNESOTA", "MISSOURI", "NEBRASKA", "NORTH DAKOTA", "SOUTH DAKOTA"), "Midwest",
                                  if_else(state %in% c("DELAWARE", "FLORIDA", "GEORGIA", "MARYLAND", "NORTH CAROLINA", "SOUTH CAROLINA", "VIRGINIA","WEST VIRGINIA", "ALABAMA", "KENTUCKY", "MISSISSIPPI", "TENNESSEE", "ARKANSAS", "LOUISIANA", "OKLAHOMA", "TEXAS"), "South", "West")))) %>%
  dummy_cols(select_columns = c("region")) %>%
  filter(totalvotes < 1000000)
```

:::

## Data Exploration {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

::: {.panel-tabset}

## Box Plot 1

<center>
```{r}
#| echo: false
house %>% 
  ggplot(aes(y=candidatevotes, x=region)) + 
  geom_boxplot() +
  theme_bw()
```
</center>

## Box Plot 2

<center>
```{r}
#| echo: false
house %>% 
  filter(candidatevotes < 450000) %>%
  ggplot(aes(y=candidatevotes, x=region)) + 
  geom_boxplot() +
  theme_bw()
```
</center>

## Histogram 1

<center>
```{r}
#| echo: false
house %>% 
  ggplot(aes(x=candidatevotes)) + 
  geom_histogram(bins = 45) +
  theme_bw()
```
</center>

## Histogram 2

<center>
```{r}
#| echo: false
house %>% 
  filter(candidatevotes < 450000) %>%
  ggplot(aes(x=candidatevotes)) + 
  geom_histogram(bins = 45) +
  theme_bw()
```
</center>

## Codes

```{r}
#| eval: false
house %>% ## BOX PLOT 1 ##
  ggplot(aes(y=candidatevotes, x=region)) + 
  geom_boxplot() +
  theme_bw()
house %>% ## BOX PLOT 2 ##
  filter(candidatevotes < 450000) %>%
  ggplot(aes(y=candidatevotes, x=region)) + 
  geom_boxplot() +
  theme_bw()
house %>% ## HISTOGRAM 1 ##
  filter(candidatevotes < 450000) %>%
  ggplot(aes(x=candidatevotes)) + 
  geom_histogram(bins = 45) +
  theme_bw()
house %>% ## HISTOGRAM 2 ##
  filter(candidatevotes < 450000) %>%
  ggplot(aes(x=candidatevotes)) + 
  geom_histogram(bins = 45) +
  theme_bw()
```

:::

## R Syntax {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We are able to again use the `glm()` function when specifying the Poisson distribution.

```{r}
#| eval: false
m <- glm(outcome ~ var_1 + var_2 + ... + var_k, 
         family="poisson",
         data=dataset)
```

- Everything we have learned about how to use model results applies here as well.

## Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let's use the election data to model the number of votes received as a function of the year, the region, the interaction between the party and the year, and the interaction between the party and region.

::: {.panel-tabset}

## Code 

```{r}
#| eval: false
m <- glm(candidatevotes ~ year10 +  party + region_Midwest + region_Northeast + region_West + 
           party:year10 + 
           party:region_Midwest + party:region_Northeast + party:region_West + totalvotes,
         family = "poisson",
         data = house)
summary(m)
```

- Why did I also include `totalvotes` in the model?

## Results

```{r}
#| echo: false
m <- glm(candidatevotes ~ year10 +  party + region_Midwest + region_Northeast + region_West + 
           party:year10 + 
           party:region_Midwest + party:region_Northeast + party:region_West + totalvotes,
         family = "poisson",
         data = house)
summary(m)
coefficients(m)
```

## Resulting Model {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

$$ 
\begin{align*}
\ln(\hat{y}) =& 12.92 - 0.01 \text{ year} - 4.17 \text{ repub.} + 0.04 \text{ midwest} + 0.11 \text{ northeast} + 0.08 \text{ west}  \\
& + 0.02 (\text{repub. $\times$ year}) \\
& - 0.06 (\text{repub. $\times$ midwest}) - 0.31 (\text{repub. $\times$ northeast}) - 0.19 (\text{repub. $\times$ west})
\end{align*}
$$

:::

## Interpretations {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- In Poisson regression, we convert the $\hat{\beta}_i$ values to incident rate ratios (IRR).

$$\text{IRR}_i = \exp\left\{\hat{\beta}_i\right\}$$ 

- This is a multiplicative effect, like an odds ratio in logistic regression. 

    - An IRR > 1 indicates an increase in the expected count. 

    - An IRR < 1 indicates a decrease in the expected count.

- We also interpret the IRR similar to the odds ratio:

  - For a 1 [unit of predictor] increase in [predictor name], the expected count of [outcome] is multiplied by  $\left[ e^{\hat{\beta}_i} \right]$. 
  
  - For a 1 [unit of predictor] increase in [predictor name], the expected count of [outcome] are [increased or decreased] by [100(e$^{\hat{\beta}_i}$-1)\% or 100(1-e$^{\hat{\beta}_i}$)\%].

## Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

::: {.panel-tabset}

## Stratifying by Party

<center><img src = "/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/1 - Teaching/STA6235 Modeling in Regression/2-Su24/lectures/images/W06-L1-a.png"></center>

## Republicans

- Every year, the expected count of votes increases by 9%.

- Those in the midwest have an expected number of votes that is 1.05 times the votes for those in the south.

    - This is a 5% increase.

- Those in the northeast have an expected number of votes that is 0.83 times the votes for those in the south.

    - This is a 17% decrease.

- Those in the west have an expected number of votes that is 0.92 times the votes for those in the south.

    - This is a 8% decrease.

## Democrats

- Every year, the expected count of votes increases by 7%.

- Those in the midwest have an expected number of votes that is 1.12 times the votes for those in the south.

    - This is a 12% increase.

- Those in the northeast have an expected number of votes that is 1.13 times the votes for those in the south.

    - This is a 13% increase.

- Those in the west have an expected number of votes that is 1.12 times the votes for those in the south.

    - This is a 12% increase.

:::

## Inference - Confidence Intervals  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We again find confidence intervals as we did before, using the `confint()` function.

```{r}
confint(m)
```

## Inference - Hypothesis Testing  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We take the same approach to determining significance as before.

- A single term in the model $\to$ `summary()`.

- Multiple terms in the model $\to$ `car::Anova()`.

    - This is *required* for a categorical variable with three or more categories.
    
## Example  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Is the interaction between party and year significant?

```{r}
summary(m)
```

- Yes! $p < 0.001$.

## Example  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Is the interaction between party and region significant?

```{r}
#| echo: false
m2 <- glm(candidatevotes ~ year10 +  party + region + party:year10 + party:region + totalvotes, family = "poisson", data = house)
car::Anova(m2, type = 3)
```

- Yes! $p < 0.001$.


## Data Visualization {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let's create a couple of data visualizations. 

    - Hold year constant at 1983.
    
    - Let total number of votes vary on the $x$-axis.
    
    - Define lines for each region...
    
        - and graphs for each politcal party.

## Data Visualization {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

::: {.panel-tabset}

## Predicted Values (set up)

```{r}
c <- coefficients(m) # get coeff

# create full model before plugging in
# c["(Intercept)"] + 
#   c["year10"]*year10 +
#   c["partyREPUBLICAN"]*repub + 
#   c["region_Midwest"]*midwest + 
#   c["region_Northeast"]*northeast + 
#   c["region_West"]*west + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*year10*repub + 
#   c["partyREPUBLICAN:region_Midwest"]*midwest*repub +
#   c["partyREPUBLICAN:region_Northeast"]*northeast*repub +
#   c["partyREPUBLICAN:region_West"]*west*repub

# Year = 1996
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*repub + 
#   c["region_Midwest"]*midwest + 
#   c["region_Northeast"]*northeast + 
#   c["region_West"]*west + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*199.6*repub + 
#   c["partyREPUBLICAN:region_Midwest"]*midwest*repub +
#   c["partyREPUBLICAN:region_Northeast"]*northeast*repub +
#   c["partyREPUBLICAN:region_West"]*west*repub

# Republicans
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*1 + 
#   c["region_Midwest"]*midwest + 
#   c["region_Northeast"]*northeast + 
#   c["region_West"]*west + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*year10*1 + 
#   c["partyREPUBLICAN:region_Midwest"]*midwest*1 +
#   c["partyREPUBLICAN:region_Northeast"]*northeast*1 +
#   c["partyREPUBLICAN:region_West"]*west*1

# Midwest
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*1 + 
#   c["region_Midwest"]*1 + 
#   c["region_Northeast"]*0 + 
#   c["region_West"]*0 + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*199.6*1 + 
#   c["partyREPUBLICAN:region_Midwest"]*1*1 +
#   c["partyREPUBLICAN:region_Northeast"]*0*1 +
#   c["partyREPUBLICAN:region_West"]*0*1

# Northeast
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*1 + 
#   c["region_Midwest"]*0 + 
#   c["region_Northeast"]*1 + 
#   c["region_West"]*0 + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*199.6*1 + 
#   c["partyREPUBLICAN:region_Midwest"]*0*1 +
#   c["partyREPUBLICAN:region_Northeast"]*1*1 +
#   c["partyREPUBLICAN:region_West"]*0*1

# West
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*1 + 
#   c["region_Midwest"]*0 + 
#   c["region_Northeast"]*0 + 
#   c["region_West"]*1 + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*199.6*1 + 
#   c["partyREPUBLICAN:region_Midwest"]*0*1 +
#   c["partyREPUBLICAN:region_Northeast"]*0*1 +
#   c["partyREPUBLICAN:region_West"]*1*1
# 
# South
# c["(Intercept)"] + 
#   c["year10"]*199.6 +
#   c["partyREPUBLICAN"]*1 + 
#   c["region_Midwest"]*0 + 
#   c["region_Northeast"]*0 + 
#   c["region_West"]*0 + 
#   c["totalvotes"]*totalvotes + 
#   c["year10:partyREPUBLICAN"]*199.6*1 + 
#   c["partyREPUBLICAN:region_Midwest"]*0*1 +
#   c["partyREPUBLICAN:region_Northeast"]*0*1 +
#   c["partyREPUBLICAN:region_West"]*0*1
```

## Predicted Values

```{r}
house <- house %>%
  mutate(r_mw = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*1 + c["region_Midwest"]*1 + c["region_Northeast"]*0 + c["region_West"]*0 +  c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*1 + c["partyREPUBLICAN:region_Midwest"]*1*1 + c["partyREPUBLICAN:region_Northeast"]*0*1 + c["partyREPUBLICAN:region_West"]*0*1),
         r_ne = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*1 + c["region_Midwest"]*0 + c["region_Northeast"]*1 + c["region_West"]*0 + c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*1 + c["partyREPUBLICAN:region_Midwest"]*0*1 + c["partyREPUBLICAN:region_Northeast"]*1*1 + c["partyREPUBLICAN:region_West"]*0*1),
         r_w = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*1 + c["region_Midwest"]*0 + c["region_Northeast"]*0 + c["region_West"]*1 + c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*1 + c["partyREPUBLICAN:region_Midwest"]*0*1 + c["partyREPUBLICAN:region_Northeast"]*0*1 + c["partyREPUBLICAN:region_West"]*1*1),
         r_s = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*1 + c["region_Midwest"]*0 + c["region_Northeast"]*0 + c["region_West"]*0 +   c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*1 + c["partyREPUBLICAN:region_Midwest"]*0*1 +c["partyREPUBLICAN:region_Northeast"]*0*1 + c["partyREPUBLICAN:region_West"]*0*1),
         d_mw = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*0 + c["region_Midwest"]*1 + c["region_Northeast"]*0 + c["region_West"]*0 +  c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*0 + c["partyREPUBLICAN:region_Midwest"]*0*1 + c["partyREPUBLICAN:region_Northeast"]*0*0 + c["partyREPUBLICAN:region_West"]*0*0),
         d_ne = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*0 + c["region_Midwest"]*0 + c["region_Northeast"]*1 + c["region_West"]*0 + c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*0 + c["partyREPUBLICAN:region_Midwest"]*0*0 + c["partyREPUBLICAN:region_Northeast"]*1*0 + c["partyREPUBLICAN:region_West"]*0*0),
         d_w = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*0 + c["region_Midwest"]*0 + c["region_Northeast"]*0 + c["region_West"]*1 + c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*0 + c["partyREPUBLICAN:region_Midwest"]*0*0 + c["partyREPUBLICAN:region_Northeast"]*0*0 + c["partyREPUBLICAN:region_West"]*1*0),
         d_s = exp(c["(Intercept)"] + c["year10"]*199.6 + c["partyREPUBLICAN"]*0 + c["region_Midwest"]*0 + c["region_Northeast"]*0 + c["region_West"]*0 +   c["totalvotes"]*totalvotes + c["year10:partyREPUBLICAN"]*199.6*0 + c["partyREPUBLICAN:region_Midwest"]*0*0 +c["partyREPUBLICAN:region_Northeast"]*0*0 + c["partyREPUBLICAN:region_West"]*0*0))
```

## Graph - Republicans

<center>
```{r}
#| echo: false

house %>% 
  ggplot(aes(x = totalvotes, y = candidatevotes)) +
  geom_point() + 
  geom_line(aes(y = r_ne), color = "pink") + 
  geom_line(aes(y = r_s), color = "hotpink") + 
  geom_line(aes(y = r_mw), color = "purple") + 
  geom_line(aes(y = r_w), color = "lightblue") +
  labs(x = "Total Numbr of Votes", y = "Votes for Candidate") +
  ggtitle("Republican Candidates") +
  theme_bw()
```
</center>

## Graph - Democrats

<center>
```{r}
#| echo: false

house %>% 
  ggplot(aes(x = totalvotes, y = candidatevotes)) +
  geom_point() + 
  geom_line(aes(y = d_ne), color = "pink") + 
  geom_line(aes(y = d_s), color = "hotpink") + 
  geom_line(aes(y = d_mw), color = "purple") + 
  geom_line(aes(y = d_w), color = "lightblue") +
  labs(x = "Total Numbr of Votes", y = "Votes for Candidate") +
  ggtitle("Democratic Candidates") +
  theme_bw()
```
</center>

## Codes

```{r}
#| eval: false

# republicans
house %>% 
  ggplot(aes(x = totalvotes, y = candidatevotes)) +
  geom_point() + 
  geom_line(aes(y = r_ne), color = "pink") + 
  geom_line(aes(y = r_s), color = "hotpink") + 
  geom_line(aes(y = r_mw), color = "purple") + 
  geom_line(aes(y = r_w), color = "lightblue") +
  labs(x = "Total Numbr of Votes", y = "Votes for Candidate") +
  ggtitle("Republican Candidates") +
  theme_bw()

# democrats
house %>% 
  ggplot(aes(x = totalvotes, y = candidatevotes)) +
  geom_point() + 
  geom_line(aes(y = d_ne), color = "pink") + 
  geom_line(aes(y = d_s), color = "hotpink") + 
  geom_line(aes(y = d_mw), color = "purple") + 
  geom_line(aes(y = d_w), color = "lightblue") +
  labs(x = "Total Numbr of Votes", y = "Votes for Candidate") +
  ggtitle("Democratic Candidates") +
  theme_bw()
```

:::

## Poisson Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- The Poisson probability mass function is as follows:
  
$$f(y; \lambda) = \frac{\lambda^y e^{-\lambda}}{y!},$$

- where $k$ is the count ($k \in \mathbb{Z}^+$).

- A requirement of this distribution is that the mean ($\lambda$) equals the variance ($\lambda$).
    
- To check this, we just find the mean and variance of the count outcome we are looking at.


## Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let's check the mean and variance of our outcome, *candidatevotes*.

```{r}
house %>% summarize(mean(candidatevotes), var(candidatevotes))
```

<center>ðŸ˜±</center>

- Poisson regression should definitely not be used!

## Negative Binomial Regression {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let us now consider negative binomial regression:
  
$$\ln\left( y \right) = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k$$

- What is different? The underlying distribution.

- We now are applying the negative binomial distribution:

$$f(y; r, p) = {y + r -1 \choose y-1} (1-p)^y p^r$$

- where $r$ is the number of successes and $p$ is the probability of success.

## Negative Binomial Regression {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Why does the negative binomial handle overdispersed data better?

- The variance is as follows,

$$\text{var}[Y] = \mu + \frac{\mu^2}{k},$$

- Here, $k$ is an additional parameter, called the dispersion parameter.

- As $k \to \infty$, $\text{nb}(r, p) \to \text{Poi}(\lambda)$.

    - That is, as the dispersion parameter increases, the negative binomial *converges in distribution* to the Poisson.

- What does this mean for us?

    - If we apply the negative binomial when the Poisson was appropriate, *we will get the same results*.
    
## Negative Binomial Regression {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Why do we care if the data is overdispersed?

- When data is overdispersed, we know that the standard error is underestimated. 

- Let's think about how test statistics are created:

$$z_i = \frac{\hat{\beta}_i}{\text{SE}_{\hat{\beta}_i}}$$

- As the standard error reduces, our test statistic increases.

- As our test statistic increases, our $p$-value decreases.

- **We are inflating the type I error rate when applying the Poisson to overdispersed data!**

## R Syntax {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We now will use the `glm.nb()` function from the `MASS` package when specifying the negative binomial distribution.

    - !! WARNING !! the `MASS` package will overwrite the `select()` function from `dplyr` (`tidyverse`).
    
    - I prefer to call it using `MASS::glm.nb()` rather than calling the `MASS` package into R.

```{r}
#| eval: false
m <- MASS:glm.nb(outcome ~ var_1 + var_2 + ... + var_k, 
            data=dataset)
```

- Note that we do not need to specify the distribution *inside* the function!

    - This function is *only* for negative binomial.

## Example {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let's reproduce the example from the Poisson lecture. 

- We will use the election data to model the number of votes received as a function of the year, the region, the interaction between the party and the year, and the interaction between the party and region.

::: {.panel-tabset}

## Code 

```{r}
#| eval: false
m <- MASS::glm.nb(candidatevotes ~ year10 +  party + region_Midwest + region_Northeast + region_West + 
           party:year10 + 
           party:region_Midwest + party:region_Northeast + party:region_West + totalvotes,
         data = house)
summary(m)
```



## Results

```{r}
#| echo: false
m <- MASS::glm.nb(candidatevotes ~ year10 +  party + region_Midwest + region_Northeast + region_West + 
           party:year10 + 
           party:region_Midwest + party:region_Northeast + party:region_West + totalvotes,
         data = house)
summary(m)
```

## Resulting Model {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

$$ 
\begin{align*}
\ln(\hat{y}) =& 16.48 - 0.03 \text{ year} - 6.25 \text{ repub.} + 0.03 \text{ midwest} + 0.09 \text{ northeast} + 0.07 \text{ west}  \\
& + 0.03 (\text{repub. $\times$ year}) \\
& - 0.04 (\text{repub. $\times$ midwest}) - 0.28 (\text{repub. $\times$ northeast}) - 0.16 (\text{repub. $\times$ west})
\end{align*}
$$

:::

## Inference - Confidence Intervals  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We again find confidence intervals as we did before, using the `confint()` function.

```{r}
confint(m)
```

## Inference - Hypothesis Testing  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We take the same approach to determining significance as before.

- A single term in the model $\to$ `summary()`.

- Multiple terms in the model $\to$ `car::Anova()`.

    - This is *required* for a categorical variable with three or more categories.
    
## Example  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Is the interaction between party and year significant?

```{r}
summary(m)
```

- Yes! $p < 0.001$.

## Example  {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Is the interaction between party and region significant?

::: {.panel-tabset}

## Results

```{r}
#| echo: false
m2 <- MASS::glm.nb(candidatevotes ~ year10 +  party + region + party:year10 + party:region + totalvotes, data = house)
car::Anova(m2, type = 3)
```

- Yes! $p < 0.001$.

## Code

```{r}
#| eval: false
m2 <- MASS::glm.nb(candidatevotes ~ year10 +  party + region + party:year10 + party:region + totalvotes, data = house)
car::Anova(m2, type = 3)
```

:::

## Comparison {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Poisson model:

$$ 
\begin{align*}
\ln(\hat{y}) =& 12.92 - 0.01 \text{ year} - 4.17 \text{ repub.} + 0.04 \text{ midwest} + 0.11 \text{ northeast} + 0.08 \text{ west}  \\
& + 0.02 (\text{repub. $\times$ year}) \\
& - 0.06 (\text{repub. $\times$ midwest}) - 0.31 (\text{repub. $\times$ northeast}) - 0.19 (\text{repub. $\times$ west})
\end{align*}
$$

- Negative binomial model:

$$ 
\begin{align*}
\ln(\hat{y}) =& 16.48 - 0.03 \text{ year} - 6.25 \text{ repub.} + 0.03 \text{ midwest} + 0.09 \text{ northeast} + 0.07 \text{ west}  \\
& + 0.03 (\text{repub. $\times$ year}) \\
& - 0.04 (\text{repub. $\times$ midwest}) - 0.28 (\text{repub. $\times$ northeast}) - 0.16 (\text{repub. $\times$ west})
\end{align*}
$$

## Wrap Up {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We now have learned modeling for count data.

    - Variance $\approx$ mean $\to$ Poisson.
    
    - Variance >>> mean $\to$ negative binomial.

- Note that there is a method to handle excessive zeros in the model: zero-inflated Poisson (ZIP) and zero-inflated negative binomial (ZINB).

    - These *jointly* model the probability of a response being a 0 or not a 0 (logistic) and the count of the response (Poi/negbin).
    
    - Zero-inflation happens elsewhere, too!







































